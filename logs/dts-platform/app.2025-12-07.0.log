2025-12-07T17:00:02.564Z  INFO 282 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : The following 2 profiles are active: "api-docs", "dev" 
2025-12-07T17:00:04.885Z DEBUG 282 --- [           main] i.m.c.u.i.logging.InternalLoggerFactory  : Using SLF4J as the default logging framework 
2025-12-07T17:00:05.240Z DEBUG 282 --- [           main] c.y.d.p.config.AsyncConfiguration        : Creating Async Task Executor 
2025-12-07T17:00:05.322Z DEBUG 282 --- [           main] c.y.d.p.config.LiquibaseConfiguration    : Configuring Liquibase 
2025-12-07T17:00:05.442Z  WARN 282 --- [platform-task-1] t.j.c.liquibase.AsyncSpringLiquibase     : Starting Liquibase asynchronously, your database might not be ready at startup! 
2025-12-07T17:00:05.522Z DEBUG 282 --- [           main] c.y.d.p.config.CacheConfiguration        : Configuring Hazelcast 
2025-12-07T17:00:05.528Z  WARN 282 --- [           main] c.h.i.impl.HazelcastInstanceFactory      : Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:_ --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED 
2025-12-07T17:00:05.551Z  WARN 282 --- [           main] c.y.d.p.config.CacheConfiguration        : No discovery service is set up, Hazelcast will run in standalone mode. 
2025-12-07T17:00:05.602Z  INFO 282 --- [           main] com.hazelcast.system.logo                : [172.18.0.6]:5701 [dev] [5.5.0] _    o    o     o     o---o   o--o o      o---o     o     o----o o--o--o_    |    |    / \       /         |     /         / \    |         |_    o----o       o     o   o----o |    o             o   o----o    |_    |    |  *     \   /           |     \       *     \       |    |_    o    o *       o o---o   o--o o----o o---o *       o o----o    o_ 
2025-12-07T17:00:05.602Z  INFO 282 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved. 
2025-12-07T17:00:05.605Z  INFO 282 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Hazelcast Platform 5.5.0 (20240725) starting at [172.18.0.6]:5701 
2025-12-07T17:00:05.606Z  INFO 282 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Cluster name: dev 
2025-12-07T17:00:05.606Z  INFO 282 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker. 
2025-12-07T17:00:05.608Z  INFO 282 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] The Jet engine is disabled._To enable the Jet engine on the members, do one of the following:_  - Change member config using Java API: config.getJetConfig().setEnabled(true)_  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true_  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)_  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load) 
2025-12-07T17:00:06.069Z  INFO 282 --- [           main] com.hazelcast.system.security            : [172.18.0.6]:5701 [dev] [5.5.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see ðŸ”’ security recommendations and the status of current config. 
2025-12-07T17:00:06.134Z  INFO 282 --- [           main] com.hazelcast.instance.impl.Node         : [172.18.0.6]:5701 [dev] [5.5.0] Using Multicast discovery 
2025-12-07T17:00:06.364Z  INFO 282 --- [           main] c.h.internal.diagnostics.Diagnostics     : [172.18.0.6]:5701 [dev] [5.5.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments. 
2025-12-07T17:00:06.368Z  INFO 282 --- [           main] com.hazelcast.core.LifecycleService      : [172.18.0.6]:5701 [dev] [5.5.0] [172.18.0.6]:5701 is STARTING 
2025-12-07T17:00:06.382Z DEBUG 282 --- [platform-task-1] t.j.c.liquibase.AsyncSpringLiquibase     : Liquibase has updated your database in 939 ms 
2025-12-07T17:00:09.327Z  INFO 282 --- [           main] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:1} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T17:00:09.335Z  INFO 282 --- [           main] com.hazelcast.core.LifecycleService      : [172.18.0.6]:5701 [dev] [5.5.0] [172.18.0.6]:5701 is STARTED 
2025-12-07T17:00:10.348Z  INFO 282 --- [           main] c.h.h.HazelcastCacheRegionFactory        : Starting up HazelcastCacheRegionFactory 
2025-12-07T17:00:10.351Z  INFO 282 --- [           main] c.h.h.i.IHazelcastInstanceFactory        : Using existing HazelcastInstance [dtsPlatform]. 
2025-12-07T17:00:12.725Z  INFO 282 --- [           main] c.y.dts.common.audit.AuditActionCatalog  : Loaded 29 audit action definitions from class path resource [config/audit-action-catalog.json] 
2025-12-07T17:00:12.805Z DEBUG 282 --- [           main] c.y.dts.platform.config.WebConfigurer    : Registering CORS filter 
2025-12-07T17:00:12.853Z  INFO 282 --- [           main] c.y.dts.platform.config.WebConfigurer    : Web application configuration, using profiles: api-docs 
2025-12-07T17:00:12.854Z  INFO 282 --- [           main] c.y.dts.platform.config.WebConfigurer    : Web application fully configured 
2025-12-07T17:00:13.438Z  INFO 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : External JDBC driver loading enabled; dir=/opt/dts/drivers, explicitClass=org.apache.hive.jdbc.HiveDriver 
2025-12-07T17:00:13.440Z  INFO 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Preparing external JDBC loader with 1 jar(s); first=file:/opt/dts/drivers/quark-driver-8.37.3.jar dir=/opt/dts/drivers 
2025-12-07T17:00:13.444Z  WARN 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Failed to load explicit driver org.apache.hive.jdbc.HiveDriver: java.lang.ClassNotFoundException: org.apache.hive.jdbc.HiveDriver 
2025-12-07T17:00:13.460Z  INFO 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Registered JDBC driver via shim: io.transwarp.jdbc.QuarkDriver (delegate loader=java.net.URLClassLoader@36f51f69) 
2025-12-07T17:00:13.460Z  INFO 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Discovered and registered JDBC driver via ServiceLoader: io.transwarp.jdbc.QuarkDriver (loader=java.net.URLClassLoader@36f51f69) 
2025-12-07T17:00:13.461Z  INFO 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : DriverManager registered: org.postgresql.Driver via jdk.internal.loader.ClassLoaders$AppClassLoader@11524711 
2025-12-07T17:00:13.461Z  INFO 282 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : DriverManager registered: com.yuzhi.dts.platform.service.infra.HiveConnectionService$DriverShim via jdk.internal.loader.ClassLoaders$AppClassLoader@11524711 
2025-12-07T17:00:13.527Z  WARN 282 --- [           main] c.y.d.p.s.infra.InfraSecretService       : dts.platform.infra.encryption-key is not configured; secret fields will not be persisted 
2025-12-07T17:00:13.661Z DEBUG 282 --- [           main] org.hibernate.SQL                        : select ids1_0.id,ids1_0.created_by,ids1_0.created_date,ids1_0.description,ids1_0.jdbc_url,ids1_0.last_modified_by,ids1_0.last_modified_date,ids1_0.last_verified_at,ids1_0.name,ids1_0.props,ids1_0.secure_iv,ids1_0.secure_key_version,ids1_0.secure_props,ids1_0.status,ids1_0.type,ids1_0.username from infra_data_source ids1_0 where upper(ids1_0.type)=upper(?) and upper(ids1_0.status)=upper(?) fetch first ? rows only 
2025-12-07T17:00:21.874Z DEBUG 282 --- [           main] c.y.d.p.service.infra.AdminInfraClient   : Failed to fetch Inceptor data source from admin service at http://dts-admin:8081/api/platform/infra/inceptor: I/O error on GET request for "http://dts-admin:8081/api/platform/infra/inceptor": dts-admin: Temporary failure in name resolution 
2025-12-07T17:00:21.885Z DEBUG 282 --- [           main] c.y.d.p.service.infra.AdminInfraClient   : Failed to fetch Inceptor data source from admin service at http://localhost:8081/api/platform/infra/inceptor: I/O error on GET request for "http://localhost:8081/api/platform/infra/inceptor": Connect to http://localhost:8081 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused 
2025-12-07T17:00:21.886Z  WARN 282 --- [           main] c.y.d.p.service.infra.AdminInfraClient   : Unable to fetch active Inceptor configuration from any configured admin endpoints [http://dts-admin:8081, http://localhost:8081] 
2025-12-07T17:00:21.887Z  INFO 282 --- [           main] c.y.d.p.s.i.InceptorDataSourceRegistry   : No active Inceptor data source found. Clearing runtime registry state. 
2025-12-07T17:00:21.887Z DEBUG 282 --- [           main] c.y.d.p.s.i.InceptorDataSourceRegistry   : Cleared Inceptor registry cache (no active local or remote Inceptor data source) 
2025-12-07T17:00:23.568Z DEBUG 282 --- [           main] c.y.d.p.config.CacheConfiguration        : Starting HazelcastCacheManager 
2025-12-07T17:00:23.698Z  INFO 282 --- [           main] c.y.d.p.s.keycloak.KeycloakAuthService   : Keycloak OIDC endpoints: issuer=https://sso.dts.local/realms/S10, token=https://sso.dts.local/realms/S10/protocol/openid-connect/token, userinfo=https://sso.dts.local/realms/S10/protocol/openid-connect/userinfo 
2025-12-07T17:00:26.839Z DEBUG 282 --- [           main] c.a.JHipsterSpringDocGroupsConfiguration : Initializing JHipster OpenApi customizer 
2025-12-07T17:00:26.855Z DEBUG 282 --- [           main] c.a.JHipsterSpringDocGroupsConfiguration : Initializing JHipster OpenApi default group 
2025-12-07T17:00:26.861Z DEBUG 282 --- [           main] c.a.JHipsterSpringDocGroupsConfiguration : Initializing JHipster OpenApi management group 
2025-12-07T17:00:27.793Z DEBUG 282 --- [           main] .s.ServiceDependencyAuthenticationFilter : Filter 'serviceDependencyAuthenticationFilter' configured for use 
2025-12-07T17:00:27.793Z DEBUG 282 --- [           main] .y.d.p.s.s.PortalSessionInactivityFilter : Filter 'portalSessionInactivityFilter' configured for use 
2025-12-07T17:00:27.793Z DEBUG 282 --- [           main] c.y.d.p.web.filter.AuditLoggingFilter    : Filter 'auditLoggingFilter' configured for use 
2025-12-07T17:00:27.828Z  INFO 282 --- [           main] org.jboss.threads                        : JBoss Threads version 3.5.0.Final 
2025-12-07T17:00:27.930Z  INFO 282 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : Started DtsPlatformApp in 26.254 seconds (process running for 26.705) 
2025-12-07T17:00:27.937Z  INFO 282 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : 
----------------------------------------------------------
	Application 'dtsPlatform' is running! Access URLs:
	Local: 		http://localhost:8081/
	External: 	http://172.18.0.6:8081/
	Profile(s): 	[api-docs, dev]
---------------------------------------------------------- 
2025-12-07T17:00:27.937Z  INFO 282 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : 
----------------------------------------------------------
	Config Server: 	Not found or not setup for this application
---------------------------------------------------------- 
2025-12-07T17:00:27.950Z  INFO 282 --- [or-startup-sync] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Initializing cluster partition table arrangement... 
2025-12-07T17:00:27.990Z DEBUG 282 --- [or-startup-sync] org.hibernate.SQL                        : select ids1_0.id,ids1_0.created_by,ids1_0.created_date,ids1_0.description,ids1_0.jdbc_url,ids1_0.last_modified_by,ids1_0.last_modified_date,ids1_0.last_verified_at,ids1_0.name,ids1_0.props,ids1_0.secure_iv,ids1_0.secure_key_version,ids1_0.secure_props,ids1_0.status,ids1_0.type,ids1_0.username from infra_data_source ids1_0 where upper(ids1_0.type)=upper(?) and upper(ids1_0.status)=upper(?) fetch first ? rows only 
2025-12-07T17:00:27.993Z  WARN 282 --- [or-startup-sync] c.y.d.p.s.i.InceptorCatalogSyncService   : Skipping catalog sync: no active Inceptor or PostgreSQL data source detected 
2025-12-07T17:00:28.008Z DEBUG 282 --- [or-startup-sync] org.hibernate.SQL                        : select count(*) from catalog_dataset cd1_0 
2025-12-07T17:00:28.014Z  INFO 282 --- [or-startup-sync] y.d.p.s.i.InceptorIntegrationCoordinator : Inceptor integration synchronized. reason=startup-auto, actions=[Cleared sqlCatalogTree cache, Skipped catalog sync (no active Inceptor data source)], datasets=0 
2025-12-07T17:03:36.136Z  INFO 282 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:58367 
2025-12-07T17:03:36.219Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:2} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - 28571e41-7828-4b5a-945c-4a70fe9e051a_]_ 
2025-12-07T17:03:36.482Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T17:03:36.611Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:03:36 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=271) 
2025-12-07T17:03:39.876Z  INFO 282 --- [ration.thread-0] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Shutdown request of Member [172.18.0.9]:5701 - 28571e41-7828-4b5a-945c-4a70fe9e051a is handled 
2025-12-07T17:03:39.880Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 135 
2025-12-07T17:03:39.958Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:03:39 UTC 2025, plannedMigrations=135, completedMigrations=135, remainingMigrations=0, totalCompletedMigrations=406) 
2025-12-07T17:03:39.973Z  INFO 282 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=1, /172.18.0.6:5701->/172.18.0.9:58367, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=28571e41-7828-4b5a-945c-4a70fe9e051a, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T17:03:39.987Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:03:39.989Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:03:40.091Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:03:40.092Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:03:40.193Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:03:40.194Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:03:40.296Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:03:40.297Z  INFO 282 --- [cached.thread-1] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:03:40.297Z  WARN 282 --- [cached.thread-1] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T17:03:40.298Z  INFO 282 --- [cached.thread-1] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - 28571e41-7828-4b5a-945c-4a70fe9e051a 
2025-12-07T17:03:40.300Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T17:03:40.300Z  INFO 282 --- [cached.thread-1] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:3} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T17:03:40.315Z  INFO 282 --- [cached.thread-3] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: 28571e41-7828-4b5a-945c-4a70fe9e051a 
2025-12-07T17:06:30.660Z  INFO 282 --- [.IO.thread-in-2] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:42995 
2025-12-07T17:06:30.750Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:4} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - 175d5b82-8bd1-464c-9ac0-13b2a13672e3_]_ 
2025-12-07T17:06:31.001Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T17:06:31.123Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:06:31 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=677) 
2025-12-07T17:13:33.901Z  INFO 282 --- [.IO.thread-in-2] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=2, /172.18.0.6:5701->/172.18.0.9:42995, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=175d5b82-8bd1-464c-9ac0-13b2a13672e3, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T17:13:33.904Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:13:33.905Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:13:34.023Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:13:34.024Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:13:34.126Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:13:34.127Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:13:34.237Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:13:34.238Z  INFO 282 --- [cached.thread-4] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:13:34.239Z  WARN 282 --- [cached.thread-4] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T17:13:34.239Z  INFO 282 --- [cached.thread-4] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - 175d5b82-8bd1-464c-9ac0-13b2a13672e3 
2025-12-07T17:13:34.240Z  INFO 282 --- [cached.thread-4] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID 175d5b82-8bd1-464c-9ac0-13b2a13672e3 
2025-12-07T17:13:34.241Z  INFO 282 --- [cached.thread-4] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:5} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T17:13:34.241Z  INFO 282 --- [cached.thread-2] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: 175d5b82-8bd1-464c-9ac0-13b2a13672e3 
2025-12-07T17:13:34.255Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T17:14:28.706Z  INFO 282 --- [.IO.thread-in-1] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:40781 
2025-12-07T17:14:28.792Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:6} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - 81556ab1-16b2-4631-9eb6-ef9a33e77bf2_]_ 
2025-12-07T17:14:29.043Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T17:14:29.126Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:14:29 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=948) 
2025-12-07T17:21:20.054Z  INFO 282 --- [.IO.thread-in-1] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=3, /172.18.0.6:5701->/172.18.0.9:40781, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=81556ab1-16b2-4631-9eb6-ef9a33e77bf2, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T17:21:20.057Z  INFO 282 --- [cached.thread-6] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.057Z  INFO 282 --- [cached.thread-6] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.175Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.175Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.254Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.255Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.295Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.295Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.325Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.326Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.364Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.365Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.415Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:21:20.415Z  INFO 282 --- [ached.thread-14] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:21:20.416Z  WARN 282 --- [ached.thread-14] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T17:21:20.416Z  INFO 282 --- [ached.thread-14] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - 81556ab1-16b2-4631-9eb6-ef9a33e77bf2 
2025-12-07T17:21:20.416Z  INFO 282 --- [ached.thread-14] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID 81556ab1-16b2-4631-9eb6-ef9a33e77bf2 
2025-12-07T17:21:20.416Z  INFO 282 --- [ached.thread-14] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:7} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T17:21:20.416Z  INFO 282 --- [ached.thread-15] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: 81556ab1-16b2-4631-9eb6-ef9a33e77bf2 
2025-12-07T17:21:20.425Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T17:21:45.111Z  INFO 282 --- [.IO.thread-in-1] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:52043 
2025-12-07T17:21:45.201Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:8} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - ce42db96-cade-4773-82e2-59c7be7627d5_]_ 
2025-12-07T17:21:45.453Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T17:21:45.554Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:21:45 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=1219) 
2025-12-07T17:45:00.920Z  INFO 282 --- [.IO.thread-in-1] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=4, /172.18.0.6:5701->/172.18.0.9:52043, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=ce42db96-cade-4773-82e2-59c7be7627d5, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T17:45:00.923Z  INFO 282 --- [cached.thread-6] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:45:00.924Z  INFO 282 --- [cached.thread-6] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:45:00.933Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:45:00.934Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:45:01.026Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:45:01.027Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:45:01.129Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:45:01.131Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:45:01.167Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:45:01.167Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:45:01.232Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:45:01.233Z  INFO 282 --- [ached.thread-10] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:45:01.233Z  WARN 282 --- [ached.thread-10] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T17:45:01.233Z  INFO 282 --- [ached.thread-10] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - ce42db96-cade-4773-82e2-59c7be7627d5 
2025-12-07T17:45:01.233Z  INFO 282 --- [ached.thread-10] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID ce42db96-cade-4773-82e2-59c7be7627d5 
2025-12-07T17:45:01.234Z  INFO 282 --- [ached.thread-10] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:9} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T17:45:01.234Z  INFO 282 --- [cached.thread-6] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: ce42db96-cade-4773-82e2-59c7be7627d5 
2025-12-07T17:45:01.247Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T17:45:38.229Z  INFO 282 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:54469 
2025-12-07T17:45:38.318Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:10} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - 3f1811db-90c6-4fc9-9ead-b15fe7663828_]_ 
2025-12-07T17:45:38.570Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T17:45:38.657Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:45:38 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=1490) 
2025-12-07T17:50:06.299Z  INFO 282 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=5, /172.18.0.6:5701->/172.18.0.9:54469, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=3f1811db-90c6-4fc9-9ead-b15fe7663828, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T17:50:06.301Z  INFO 282 --- [ached.thread-13] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:50:06.301Z  INFO 282 --- [ached.thread-13] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:50:06.413Z  INFO 282 --- [ached.thread-13] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:50:06.414Z  INFO 282 --- [ached.thread-13] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:50:06.526Z  INFO 282 --- [ached.thread-11] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:50:06.527Z  INFO 282 --- [ached.thread-11] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:50:06.638Z  INFO 282 --- [ached.thread-11] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T17:50:06.639Z  INFO 282 --- [ached.thread-11] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T17:50:06.639Z  WARN 282 --- [ached.thread-11] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T17:50:06.639Z  INFO 282 --- [ached.thread-11] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - 3f1811db-90c6-4fc9-9ead-b15fe7663828 
2025-12-07T17:50:06.640Z  INFO 282 --- [ached.thread-11] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID 3f1811db-90c6-4fc9-9ead-b15fe7663828 
2025-12-07T17:50:06.640Z  INFO 282 --- [ached.thread-11] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:11} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T17:50:06.640Z  INFO 282 --- [cached.thread-5] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: 3f1811db-90c6-4fc9-9ead-b15fe7663828 
2025-12-07T17:50:06.652Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T17:53:14.118Z  INFO 282 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:51029 
2025-12-07T17:53:14.206Z  INFO 282 --- [ration.thread-6] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:12} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - 8a6b8231-a776-48db-a16c-2084229f968b_]_ 
2025-12-07T17:53:14.458Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T17:53:14.544Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 17:53:14 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=1761) 
2025-12-07T18:03:18.815Z  INFO 282 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=6, /172.18.0.6:5701->/172.18.0.9:51029, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=8a6b8231-a776-48db-a16c-2084229f968b, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T18:03:18.818Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:03:18.819Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:03:18.933Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:03:18.933Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:03:19.044Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:03:19.045Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:03:19.156Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:03:19.157Z  INFO 282 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:03:19.157Z  WARN 282 --- [cached.thread-3] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T18:03:19.158Z  INFO 282 --- [cached.thread-3] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - 8a6b8231-a776-48db-a16c-2084229f968b 
2025-12-07T18:03:19.158Z  INFO 282 --- [cached.thread-3] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID 8a6b8231-a776-48db-a16c-2084229f968b 
2025-12-07T18:03:19.160Z  INFO 282 --- [cached.thread-3] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:13} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T18:03:19.160Z  INFO 282 --- [cached.thread-9] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: 8a6b8231-a776-48db-a16c-2084229f968b 
2025-12-07T18:03:19.175Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T18:03:56.601Z  INFO 282 --- [.IO.thread-in-2] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:42419 
2025-12-07T18:03:56.691Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:14} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - b39ef526-d08a-4add-8873-5f0760a0d3fd_]_ 
2025-12-07T18:03:56.942Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T18:03:57.025Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 18:03:56 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=2032) 
2025-12-07T18:04:51.114Z DEBUG 282 --- [rm-scheduling-1] org.hibernate.SQL                        : select rs1_0.id,rs1_0.chunk_count,rs1_0.columns,rs1_0.created_by,rs1_0.created_date,rs1_0.expires_at,rs1_0.last_modified_by,rs1_0.last_modified_date,rs1_0.name,rs1_0.preview_columns,rs1_0.row_count,rs1_0.storage_format,rs1_0.storage_uri,rs1_0.ttl_days from result_set rs1_0 where rs1_0.expires_at<? 
2025-12-07T18:09:36.797Z  INFO 282 --- [.IO.thread-in-2] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=7, /172.18.0.6:5701->/172.18.0.9:42419, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=b39ef526-d08a-4add-8873-5f0760a0d3fd, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T18:09:36.799Z  INFO 282 --- [ached.thread-17] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:09:36.800Z  INFO 282 --- [ached.thread-17] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:09:36.912Z  INFO 282 --- [cached.thread-8] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:09:36.913Z  INFO 282 --- [cached.thread-8] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:09:37.015Z  INFO 282 --- [cached.thread-8] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:09:37.016Z  INFO 282 --- [cached.thread-8] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:09:37.125Z  INFO 282 --- [ached.thread-17] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:09:37.126Z  INFO 282 --- [ached.thread-17] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:09:37.126Z  WARN 282 --- [ached.thread-17] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T18:09:37.126Z  INFO 282 --- [ached.thread-17] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - b39ef526-d08a-4add-8873-5f0760a0d3fd 
2025-12-07T18:09:37.127Z  INFO 282 --- [ached.thread-17] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID b39ef526-d08a-4add-8873-5f0760a0d3fd 
2025-12-07T18:09:37.128Z  INFO 282 --- [ached.thread-17] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:15} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this_]_ 
2025-12-07T18:09:37.128Z  INFO 282 --- [cached.thread-8] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: b39ef526-d08a-4add-8873-5f0760a0d3fd 
2025-12-07T18:09:37.143Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T18:10:44.320Z  INFO 282 --- [.IO.thread-in-1] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:57995 
2025-12-07T18:10:44.409Z  INFO 282 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:16} [__Member [172.18.0.6]:5701 - 97f005f1-1032-4782-ba15-7ee7108b903c this__Member [172.18.0.9]:5701 - 01fa337e-9acf-464b-a151-31f185c9fe4c_]_ 
2025-12-07T18:10:44.661Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T18:10:44.746Z  INFO 282 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 18:10:44 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=2303) 
2025-12-07T18:14:11.149Z  INFO 273 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : The following 2 profiles are active: "api-docs", "dev" 
2025-12-07T18:14:14.081Z DEBUG 273 --- [           main] i.m.c.u.i.logging.InternalLoggerFactory  : Using SLF4J as the default logging framework 
2025-12-07T18:14:14.467Z DEBUG 273 --- [           main] c.y.d.p.config.AsyncConfiguration        : Creating Async Task Executor 
2025-12-07T18:14:14.553Z DEBUG 273 --- [           main] c.y.d.p.config.LiquibaseConfiguration    : Configuring Liquibase 
2025-12-07T18:14:14.677Z  WARN 273 --- [platform-task-1] t.j.c.liquibase.AsyncSpringLiquibase     : Starting Liquibase asynchronously, your database might not be ready at startup! 
2025-12-07T18:14:14.757Z DEBUG 273 --- [           main] c.y.d.p.config.CacheConfiguration        : Configuring Hazelcast 
2025-12-07T18:14:14.766Z  WARN 273 --- [           main] c.h.i.impl.HazelcastInstanceFactory      : Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:_ --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED 
2025-12-07T18:14:14.790Z  WARN 273 --- [           main] c.y.d.p.config.CacheConfiguration        : No discovery service is set up, Hazelcast will run in standalone mode. 
2025-12-07T18:14:14.856Z  INFO 273 --- [           main] com.hazelcast.system.logo                : [172.18.0.6]:5701 [dev] [5.5.0] _    o    o     o     o---o   o--o o      o---o     o     o----o o--o--o_    |    |    / \       /         |     /         / \    |         |_    o----o       o     o   o----o |    o             o   o----o    |_    |    |  *     \   /           |     \       *     \       |    |_    o    o *       o o---o   o--o o----o o---o *       o o----o    o_ 
2025-12-07T18:14:14.857Z  INFO 273 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved. 
2025-12-07T18:14:14.860Z  INFO 273 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Hazelcast Platform 5.5.0 (20240725) starting at [172.18.0.6]:5701 
2025-12-07T18:14:14.860Z  INFO 273 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Cluster name: dev 
2025-12-07T18:14:14.860Z  INFO 273 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker. 
2025-12-07T18:14:14.862Z  INFO 273 --- [           main] com.hazelcast.system                     : [172.18.0.6]:5701 [dev] [5.5.0] The Jet engine is disabled._To enable the Jet engine on the members, do one of the following:_  - Change member config using Java API: config.getJetConfig().setEnabled(true)_  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true_  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)_  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load) 
2025-12-07T18:14:15.307Z  INFO 273 --- [           main] com.hazelcast.system.security            : [172.18.0.6]:5701 [dev] [5.5.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see ðŸ”’ security recommendations and the status of current config. 
2025-12-07T18:14:15.365Z  INFO 273 --- [           main] com.hazelcast.instance.impl.Node         : [172.18.0.6]:5701 [dev] [5.5.0] Using Multicast discovery 
2025-12-07T18:14:15.576Z  INFO 273 --- [           main] c.h.internal.diagnostics.Diagnostics     : [172.18.0.6]:5701 [dev] [5.5.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments. 
2025-12-07T18:14:15.583Z  INFO 273 --- [           main] com.hazelcast.core.LifecycleService      : [172.18.0.6]:5701 [dev] [5.5.0] [172.18.0.6]:5701 is STARTING 
2025-12-07T18:14:15.628Z DEBUG 273 --- [platform-task-1] t.j.c.liquibase.AsyncSpringLiquibase     : Liquibase has updated your database in 950 ms 
2025-12-07T18:14:15.729Z  INFO 273 --- [           main] c.h.i.cluster.impl.MulticastJoiner       : [172.18.0.6]:5701 [dev] [5.5.0] Trying to join to discovered node: [172.18.0.9]:5701 
2025-12-07T18:14:15.746Z  INFO 273 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:34743 and /172.18.0.9:5701 
2025-12-07T18:14:15.844Z  INFO 273 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:18} [__Member [172.18.0.9]:5701 - 01fa337e-9acf-464b-a151-31f185c9fe4c__Member [172.18.0.6]:5701 - 243cf81e-0ce0-4f6b-8253-f21621f67727 this_]_ 
2025-12-07T18:14:15.852Z  INFO 273 --- [           main] com.hazelcast.core.LifecycleService      : [172.18.0.6]:5701 [dev] [5.5.0] [172.18.0.6]:5701 is STARTED 
2025-12-07T18:14:16.878Z  INFO 273 --- [           main] c.h.h.HazelcastCacheRegionFactory        : Starting up HazelcastCacheRegionFactory 
2025-12-07T18:14:16.880Z  INFO 273 --- [           main] c.h.h.i.IHazelcastInstanceFactory        : Using existing HazelcastInstance [dtsPlatform]. 
2025-12-07T18:14:18.913Z  INFO 273 --- [           main] c.y.dts.common.audit.AuditActionCatalog  : Loaded 29 audit action definitions from class path resource [config/audit-action-catalog.json] 
2025-12-07T18:14:18.983Z DEBUG 273 --- [           main] c.y.dts.platform.config.WebConfigurer    : Registering CORS filter 
2025-12-07T18:14:19.019Z  INFO 273 --- [           main] c.y.dts.platform.config.WebConfigurer    : Web application configuration, using profiles: api-docs 
2025-12-07T18:14:19.019Z  INFO 273 --- [           main] c.y.dts.platform.config.WebConfigurer    : Web application fully configured 
2025-12-07T18:14:19.501Z  INFO 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : External JDBC driver loading enabled; dir=/opt/dts/drivers, explicitClass=org.apache.hive.jdbc.HiveDriver 
2025-12-07T18:14:19.503Z  INFO 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Preparing external JDBC loader with 1 jar(s); first=file:/opt/dts/drivers/quark-driver-8.37.3.jar dir=/opt/dts/drivers 
2025-12-07T18:14:19.507Z  WARN 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Failed to load explicit driver org.apache.hive.jdbc.HiveDriver: java.lang.ClassNotFoundException: org.apache.hive.jdbc.HiveDriver 
2025-12-07T18:14:19.517Z  INFO 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Registered JDBC driver via shim: io.transwarp.jdbc.QuarkDriver (delegate loader=java.net.URLClassLoader@12ec2e89) 
2025-12-07T18:14:19.518Z  INFO 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : Discovered and registered JDBC driver via ServiceLoader: io.transwarp.jdbc.QuarkDriver (loader=java.net.URLClassLoader@12ec2e89) 
2025-12-07T18:14:19.519Z  INFO 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : DriverManager registered: org.postgresql.Driver via jdk.internal.loader.ClassLoaders$AppClassLoader@11524711 
2025-12-07T18:14:19.520Z  INFO 273 --- [           main] c.y.d.p.s.infra.HiveConnectionService    : DriverManager registered: com.yuzhi.dts.platform.service.infra.HiveConnectionService$DriverShim via jdk.internal.loader.ClassLoaders$AppClassLoader@11524711 
2025-12-07T18:14:19.574Z  WARN 273 --- [           main] c.y.d.p.s.infra.InfraSecretService       : dts.platform.infra.encryption-key is not configured; secret fields will not be persisted 
2025-12-07T18:14:19.678Z DEBUG 273 --- [           main] org.hibernate.SQL                        : select ids1_0.id,ids1_0.created_by,ids1_0.created_date,ids1_0.description,ids1_0.jdbc_url,ids1_0.last_modified_by,ids1_0.last_modified_date,ids1_0.last_verified_at,ids1_0.name,ids1_0.props,ids1_0.secure_iv,ids1_0.secure_key_version,ids1_0.secure_props,ids1_0.status,ids1_0.type,ids1_0.username from infra_data_source ids1_0 where upper(ids1_0.type)=upper(?) and upper(ids1_0.status)=upper(?) fetch first ? rows only 
2025-12-07T18:14:19.869Z DEBUG 273 --- [           main] c.y.d.p.service.infra.AdminInfraClient   : Admin infra endpoint http://dts-admin:8081/api/platform/infra/inceptor returned status 204 NO_CONTENT 
2025-12-07T18:14:19.894Z DEBUG 273 --- [           main] c.y.d.p.service.infra.AdminInfraClient   : Failed to fetch Inceptor data source from admin service at http://localhost:8081/api/platform/infra/inceptor: I/O error on GET request for "http://localhost:8081/api/platform/infra/inceptor": Connect to http://localhost:8081 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused 
2025-12-07T18:14:19.894Z  WARN 273 --- [           main] c.y.d.p.service.infra.AdminInfraClient   : Unable to fetch active Inceptor configuration from any configured admin endpoints [http://dts-admin:8081, http://localhost:8081] 
2025-12-07T18:14:19.895Z  INFO 273 --- [           main] c.y.d.p.s.i.InceptorDataSourceRegistry   : No active Inceptor data source found. Clearing runtime registry state. 
2025-12-07T18:14:19.895Z DEBUG 273 --- [           main] c.y.d.p.s.i.InceptorDataSourceRegistry   : Cleared Inceptor registry cache (no active local or remote Inceptor data source) 
2025-12-07T18:14:21.767Z DEBUG 273 --- [           main] c.y.d.p.config.CacheConfiguration        : Starting HazelcastCacheManager 
2025-12-07T18:14:21.892Z  INFO 273 --- [           main] c.y.d.p.s.keycloak.KeycloakAuthService   : Keycloak OIDC endpoints: issuer=https://sso.dts.local/realms/S10, token=https://sso.dts.local/realms/S10/protocol/openid-connect/token, userinfo=https://sso.dts.local/realms/S10/protocol/openid-connect/userinfo 
2025-12-07T18:14:25.044Z DEBUG 273 --- [           main] c.a.JHipsterSpringDocGroupsConfiguration : Initializing JHipster OpenApi customizer 
2025-12-07T18:14:25.065Z DEBUG 273 --- [           main] c.a.JHipsterSpringDocGroupsConfiguration : Initializing JHipster OpenApi default group 
2025-12-07T18:14:25.068Z DEBUG 273 --- [           main] c.a.JHipsterSpringDocGroupsConfiguration : Initializing JHipster OpenApi management group 
2025-12-07T18:14:26.064Z DEBUG 273 --- [           main] .s.ServiceDependencyAuthenticationFilter : Filter 'serviceDependencyAuthenticationFilter' configured for use 
2025-12-07T18:14:26.065Z DEBUG 273 --- [           main] .y.d.p.s.s.PortalSessionInactivityFilter : Filter 'portalSessionInactivityFilter' configured for use 
2025-12-07T18:14:26.066Z DEBUG 273 --- [           main] c.y.d.p.web.filter.AuditLoggingFilter    : Filter 'auditLoggingFilter' configured for use 
2025-12-07T18:14:26.107Z  INFO 273 --- [           main] org.jboss.threads                        : JBoss Threads version 3.5.0.Final 
2025-12-07T18:14:26.192Z  INFO 273 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : Started DtsPlatformApp in 16.144 seconds (process running for 16.854) 
2025-12-07T18:14:26.198Z  INFO 273 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : 
----------------------------------------------------------
	Application 'dtsPlatform' is running! Access URLs:
	Local: 		http://localhost:8081/
	External: 	http://172.18.0.6:8081/
	Profile(s): 	[api-docs, dev]
---------------------------------------------------------- 
2025-12-07T18:14:26.198Z  INFO 273 --- [           main] com.yuzhi.dts.platform.DtsPlatformApp    : 
----------------------------------------------------------
	Config Server: 	Not found or not setup for this application
---------------------------------------------------------- 
2025-12-07T18:14:26.224Z DEBUG 273 --- [or-startup-sync] org.hibernate.SQL                        : select ids1_0.id,ids1_0.created_by,ids1_0.created_date,ids1_0.description,ids1_0.jdbc_url,ids1_0.last_modified_by,ids1_0.last_modified_date,ids1_0.last_verified_at,ids1_0.name,ids1_0.props,ids1_0.secure_iv,ids1_0.secure_key_version,ids1_0.secure_props,ids1_0.status,ids1_0.type,ids1_0.username from infra_data_source ids1_0 where upper(ids1_0.type)=upper(?) and upper(ids1_0.status)=upper(?) fetch first ? rows only 
2025-12-07T18:14:26.227Z  WARN 273 --- [or-startup-sync] c.y.d.p.s.i.InceptorCatalogSyncService   : Skipping catalog sync: no active Inceptor or PostgreSQL data source detected 
2025-12-07T18:14:26.241Z DEBUG 273 --- [or-startup-sync] org.hibernate.SQL                        : select count(*) from catalog_dataset cd1_0 
2025-12-07T18:14:26.246Z  INFO 273 --- [or-startup-sync] y.d.p.s.i.InceptorIntegrationCoordinator : Inceptor integration synchronized. reason=startup-auto, actions=[Cleared sqlCatalogTree cache, Skipped catalog sync (no active Inceptor data source)], datasets=0 
2025-12-07T18:14:35.549Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : Enter: login() with argument[s] = [LoginPayload[username=u-90-01, password=sa]] 
2025-12-07T18:14:35.550Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : [login] attempt username=u-90-01 
2025-12-07T18:14:35.551Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.service.admin.AdminAuthClient    : [admin-auth-client-ip] forwarded='::ffff:172.18.0.1' forwardedStd='' real='' remote='::ffff:172.18.0.1' resolved='172.18.0.1' outbound='172.18.0.1, ::ffff:172.18.0.1' outboundStd='for="172.18.0.1"' fallbackToRemote=false missingForwarded=false 
2025-12-07T18:14:35.674Z  WARN 273 --- [  XNIO-1 task-2] c.y.d.p.service.admin.AdminAuthClient    : Admin auth call POST failed: status=401 body={_  "status" : "ERROR",_  "message" : "ç”¨æˆ·å°šæœªå®¡æ‰¹å¯ç”¨ï¼Œè¯·è”ç³»æŽˆæƒç®¡ç†å‘˜",_  "data" : null_} uri=http://dts-admin:8081/api/keycloak/auth/platform/login?auditSilent=true 
2025-12-07T18:14:35.676Z  WARN 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : [login] unauthorized username=u-90-01 reason=ç”¨æˆ·å°šæœªå®¡æ‰¹å¯ç”¨ï¼Œè¯·è”ç³»æŽˆæƒç®¡ç†å‘˜ 
2025-12-07T18:14:35.676Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : Enter: recordAs() with argument[s] = [u-90-01, AUTH LOGIN, platform, portal_user, u-90-01, FAILED, {audience=platform, username=u-90-01, targetName=u-90-01, resourceName=u-90-01, summary=ä¸šåŠ¡ç«¯ç™»å½•å¤±è´¥ï¼šu-90-01, operationType=LOGIN, error=ç”¨æˆ·å°šæœªå®¡æ‰¹å¯ç”¨ï¼Œè¯·è”ç³»æŽˆæƒç®¡ç†å‘˜}, {audience=platform}] 
2025-12-07T18:14:35.677Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : AUDIT actor=u-90-01 action=AUTH LOGIN module=platform resourceType=portal_user resourceId=u-90-01 result=FAILED 
2025-12-07T18:14:35.684Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditTrailService  : Enter: record() with argument[s] = [com.yuzhi.dts.platform.service.audit.AuditTrailService$PendingAuditEvent@71b8a1f4] 
2025-12-07T18:14:35.719Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditTrailService  : Exit: record() with result = null 
2025-12-07T18:14:35.719Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : Exit: recordAs() with result = null 
2025-12-07T18:14:35.720Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : Exit: login() with result = <401 UNAUTHORIZED Unauthorized,com.yuzhi.dts.platform.web.rest.ApiResponse@32e45af3,[]> 
2025-12-07T18:27:46.567Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : Enter: login() with argument[s] = [LoginPayload[username=u-90-01, password=sa]] 
2025-12-07T18:27:46.568Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : [login] attempt username=u-90-01 
2025-12-07T18:27:46.568Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.service.admin.AdminAuthClient    : [admin-auth-client-ip] forwarded='::ffff:172.18.0.1' forwardedStd='' real='' remote='::ffff:172.18.0.1' resolved='172.18.0.1' outbound='172.18.0.1, ::ffff:172.18.0.1' outboundStd='for="172.18.0.1"' fallbackToRemote=false missingForwarded=false 
2025-12-07T18:27:46.669Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.normalized_username=? and pse1_0.revoked_at is null for no key update 
2025-12-07T18:27:46.678Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.normalized_username=? and pse1_0.revoked_at is null for no key update 
2025-12-07T18:27:46.699Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.id=? 
2025-12-07T18:27:46.775Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : insert into portal_sessions (access_token,admin_access_token,admin_access_token_expires_at,admin_refresh_token,admin_refresh_token_expires_at,created_at,dept_code,display_name,expires_at,last_seen_at,normalized_username,permissions,personnel_level,refresh_token,revoked_at,revoked_by_session_id,revoked_reason,roles,session_id,username,id) values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?) 
2025-12-07T18:27:46.787Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : [login] success username=u-90-01 roles=[ROLE_EMPLOYEE, ROLE_OFFLINE_ACCESS, ROLE_UMA_AUTHORIZATION, ROLE_DEFAULT_ROLES_S10] perms=[portal.view] 
2025-12-07T18:27:46.788Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : Enter: recordAs() with argument[s] = [u-90-01, AUTH LOGIN, platform, portal_user, u-90-01, SUCCESS, {audience=platform, username=u-90-01, actorName=u-90-01, targetName=u-90-01, resourceName=u-90-01, summary=ä¸šåŠ¡ç«¯ç™»å½•æˆåŠŸï¼šu-90-01, operationType=LOGIN}, {audience=platform}] 
2025-12-07T18:27:46.788Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : AUDIT actor=u-90-01 action=AUTH LOGIN module=platform resourceType=portal_user resourceId=u-90-01 result=SUCCESS 
2025-12-07T18:27:46.788Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditTrailService  : Enter: record() with argument[s] = [com.yuzhi.dts.platform.service.audit.AuditTrailService$PendingAuditEvent@2a2a2994] 
2025-12-07T18:27:46.801Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditTrailService  : Exit: record() with result = null 
2025-12-07T18:27:46.801Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : Exit: recordAs() with result = null 
2025-12-07T18:27:46.804Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select ids1_0.id,ids1_0.created_by,ids1_0.created_date,ids1_0.description,ids1_0.jdbc_url,ids1_0.last_modified_by,ids1_0.last_modified_date,ids1_0.last_verified_at,ids1_0.name,ids1_0.props,ids1_0.secure_iv,ids1_0.secure_key_version,ids1_0.secure_props,ids1_0.status,ids1_0.type,ids1_0.username from infra_data_source ids1_0 where upper(ids1_0.type)=upper(?) and upper(ids1_0.status)=upper(?) fetch first ? rows only 
2025-12-07T18:27:46.814Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.infra.AdminInfraClient   : Admin infra endpoint http://dts-admin:8081/api/platform/infra/inceptor returned status 204 NO_CONTENT 
2025-12-07T18:27:46.823Z DEBUG 273 --- [  XNIO-1 task-3] .s.ServiceDependencyAuthenticationFilter : Authenticated internal service call as dts-admin 
2025-12-07T18:27:46.851Z  INFO 273 --- [  XNIO-1 task-3] c.y.d.p.web.filter.AuditLoggingFilter    : [platform-client-ip] resolved= forwarded='' forwardedStd='' real='' remote='127.0.0.1' candidates=[127.0.0.1] fallbackToRemote=false missingForwarded=true 
2025-12-07T18:27:46.851Z  INFO 273 --- [  XNIO-1 task-3] c.y.d.p.web.filter.AuditLoggingFilter    : [access] GET /api/platform/infra/inceptor status=404 user=service:dts-admin ua="Apache-HttpClient/5.4.3 (Java/21.0.7)" ip=- dur=27ms 
2025-12-07T18:27:46.852Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.infra.AdminInfraClient   : Failed to fetch Inceptor data source from admin service at http://localhost:8081/api/platform/infra/inceptor: 404 Not Found on GET request for "http://localhost:8081/api/platform/infra/inceptor": "{<EOL>  "type" : "https://www.jhipster.tech/problem/problem-with-message",<EOL>  "title" : "Not Found",<EOL>  "status" : 404,<EOL>  "detail" : "No static resource api/platform/infra/inceptor.",<EOL>  "instance" : "/api/platform/infra/inceptor",<EOL>  "message" : "error.http.404",<EOL>  "path" : "/api/platform/infra/inceptor"<EOL>}" 
2025-12-07T18:27:46.852Z  WARN 273 --- [  XNIO-1 task-2] c.y.d.p.service.infra.AdminInfraClient   : Unable to fetch active Inceptor configuration from any configured admin endpoints [http://dts-admin:8081, http://localhost:8081] 
2025-12-07T18:27:46.852Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.s.i.InceptorDataSourceRegistry   : No active Inceptor data source found. Clearing runtime registry state. 
2025-12-07T18:27:46.852Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.s.i.InceptorDataSourceRegistry   : Cleared Inceptor registry cache (no active local or remote Inceptor data source) 
2025-12-07T18:27:46.852Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : Exit: login() with result = <200 OK OK,com.yuzhi.dts.platform.web.rest.ApiResponse@76287f7c,[]> 
2025-12-07T18:27:46.880Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.access_token=? 
2025-12-07T18:27:46.903Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:46.915Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.access_token=? 
2025-12-07T18:27:46.921Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:46.928Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.w.r.KeycloakLocalizationResource : Enter: zhCN() with argument[s] = [] 
2025-12-07T18:27:46.928Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.w.r.KeycloakLocalizationResource : Exit: zhCN() with result = {commonActions={save=ä¿å­˜, cancel=å–æ¶ˆ}, roleManagement={assign=åˆ†é…è§’è‰², title=è§’è‰²ç®¡ç†}, userManagement={delete=åˆ é™¤ç”¨æˆ·, title=ç”¨æˆ·ç®¡ç†, create=åˆ›å»ºç”¨æˆ·}, statusMessages={success=æ“ä½œæˆåŠŸ, error=æ“ä½œå¤±è´¥}, pagination={prev=ä¸Šä¸€é¡µ, next=ä¸‹ä¸€é¡µ}, formLabels={username=ç”¨æˆ·å, email=é‚®ç®±}, groupManagement={title=ç”¨æˆ·ç»„ç®¡ç†}} 
2025-12-07T18:27:47.177Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.access_token=? 
2025-12-07T18:27:47.181Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:47.188Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.access_token=? 
2025-12-07T18:27:47.193Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:47.198Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.BasicApiResource        : Enter: menuTree() with argument[s] = [] 
2025-12-07T18:27:47.198Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.menu.PortalMenuService   : Enter: getMenuTreeView() with argument[s] = [] 
2025-12-07T18:27:47.219Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.menu.PortalMenuClient    : Failed to fetch active menu tree via admin API: 401 Unauthorized on GET request for "http://dts-admin:8081/api/admin/portal/menus": [no body] 
2025-12-07T18:27:47.279Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.menu.PortalMenuService   : Exit: getMenuTreeView() with result = [] 
2025-12-07T18:27:47.279Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.BasicApiResource        : Exit: menuTree() with result = com.yuzhi.dts.platform.web.rest.ApiResponse@1efdb9fb 
2025-12-07T18:27:47.282Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.filter.AuditLoggingFilter    : [platform-client-ip] resolved=172.18.0.1 forwarded='::ffff:172.18.0.1' forwardedStd='' real='' remote='::ffff:172.18.0.1' candidates=[::ffff:172.18.0.1, ::ffff:172.18.0.1] fallbackToRemote=false missingForwarded=false 
2025-12-07T18:27:47.282Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.filter.AuditLoggingFilter    : [access] GET /api/menu/tree status=200 user=u-90-01 ua="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36" ip=172.18.0.1 dur=96ms 
2025-12-07T18:27:47.290Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.access_token=? 
2025-12-07T18:27:47.295Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:47.302Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.access_token=? 
2025-12-07T18:27:47.308Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:47.312Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.BasicApiResource        : Enter: menuTree() with argument[s] = [] 
2025-12-07T18:27:47.312Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.menu.PortalMenuService   : Enter: getMenuTreeView() with argument[s] = [] 
2025-12-07T18:27:47.317Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.menu.PortalMenuClient    : Failed to fetch active menu tree via admin API: 401 Unauthorized on GET request for "http://dts-admin:8081/api/admin/portal/menus": [no body] 
2025-12-07T18:27:47.356Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.menu.PortalMenuService   : Exit: getMenuTreeView() with result = [] 
2025-12-07T18:27:47.356Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.BasicApiResource        : Exit: menuTree() with result = com.yuzhi.dts.platform.web.rest.ApiResponse@4bac1671 
2025-12-07T18:27:47.358Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.filter.AuditLoggingFilter    : [platform-client-ip] resolved=172.18.0.1 forwarded='::ffff:172.18.0.1' forwardedStd='' real='' remote='::ffff:172.18.0.1' candidates=[::ffff:172.18.0.1, ::ffff:172.18.0.1] fallbackToRemote=false missingForwarded=false 
2025-12-07T18:27:47.358Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.web.filter.AuditLoggingFilter    : [access] GET /api/menu/tree status=200 user=u-90-01 ua="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36" ip=172.18.0.1 dur=57ms 
2025-12-07T18:27:52.212Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : Enter: logout() with argument[s] = [RefreshPayload[refreshToken=refresh-f62fec8b-e358-4876-8def-2f0be88e01be, username=u-90-01]] 
2025-12-07T18:27:52.213Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.refresh_token=? 
2025-12-07T18:27:52.219Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : update portal_sessions set access_token=?,admin_access_token=?,admin_access_token_expires_at=?,admin_refresh_token=?,admin_refresh_token_expires_at=?,created_at=?,dept_code=?,display_name=?,expires_at=?,last_seen_at=?,normalized_username=?,permissions=?,personnel_level=?,refresh_token=?,revoked_at=?,revoked_by_session_id=?,revoked_reason=?,roles=?,session_id=?,username=? where id=? 
2025-12-07T18:27:52.222Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.s.session.PortalSessionRegistry  : [session] revoke username=u-90-01 session=e3a13352-f66e-4644-a495-da598f8119b9 reason=LOGOUT 
2025-12-07T18:27:52.245Z DEBUG 273 --- [  XNIO-1 task-2] org.hibernate.SQL                        : select pse1_0.id,pse1_0.access_token,pse1_0.admin_access_token,pse1_0.admin_access_token_expires_at,pse1_0.admin_refresh_token,pse1_0.admin_refresh_token_expires_at,pse1_0.created_at,pse1_0.dept_code,pse1_0.display_name,pse1_0.expires_at,pse1_0.last_seen_at,pse1_0.normalized_username,pse1_0.permissions,pse1_0.personnel_level,pse1_0.refresh_token,pse1_0.revoked_at,pse1_0.revoked_by_session_id,pse1_0.revoked_reason,pse1_0.roles,pse1_0.session_id,pse1_0.username from portal_sessions pse1_0 where pse1_0.refresh_token=? 
2025-12-07T18:27:52.250Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.service.admin.AdminAuthClient    : [admin-auth-client-ip] forwarded='::ffff:172.18.0.1' forwardedStd='' real='' remote='::ffff:172.18.0.1' resolved='172.18.0.1' outbound='172.18.0.1, ::ffff:172.18.0.1' outboundStd='for="172.18.0.1"' fallbackToRemote=false missingForwarded=false 
2025-12-07T18:27:52.274Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : Enter: recordAs() with argument[s] = [u-90-01, AUTH LOGOUT, platform, portal_user, u-90-01, SUCCESS, {audience=platform, username=u-90-01, actorName=u-90-01, targetName=u-90-01, resourceName=u-90-01, summary=ä¸šåŠ¡ç«¯ç™»å‡ºæˆåŠŸï¼šu-90-01, operationType=LOGOUT, hasRefreshToken=true}, {audience=platform}] 
2025-12-07T18:27:52.274Z  INFO 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : AUDIT actor=u-90-01 action=AUTH LOGOUT module=platform resourceType=portal_user resourceId=u-90-01 result=SUCCESS 
2025-12-07T18:27:52.275Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditTrailService  : Enter: record() with argument[s] = [com.yuzhi.dts.platform.service.audit.AuditTrailService$PendingAuditEvent@63b7c525] 
2025-12-07T18:27:52.288Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditTrailService  : Exit: record() with result = null 
2025-12-07T18:27:52.288Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.service.audit.AuditService       : Exit: recordAs() with result = null 
2025-12-07T18:27:52.288Z DEBUG 273 --- [  XNIO-1 task-2] c.y.d.p.web.rest.KeycloakAuthResource    : Exit: logout() with result = <200 OK OK,com.yuzhi.dts.platform.web.rest.ApiResponse@10cf398b,[]> 
2025-12-07T18:41:18.564Z  INFO 273 --- [.IO.thread-in-0] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=1, /172.18.0.6:34743->/172.18.0.9:5701, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=01fa337e-9acf-464b-a151-31f185c9fe4c, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T18:41:18.574Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:41:18.575Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:41:18.686Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:41:18.687Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:41:18.790Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:41:18.791Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:41:18.904Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:41:18.905Z  INFO 273 --- [cached.thread-3] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:41:18.906Z  WARN 273 --- [cached.thread-3] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T18:41:18.907Z  WARN 273 --- [cached.thread-3] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Member [172.18.0.9]:5701 - 01fa337e-9acf-464b-a151-31f185c9fe4c is suspected to be dead for reason: No connection 
2025-12-07T18:41:18.907Z  INFO 273 --- [cached.thread-3] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Starting mastership claim process... 
2025-12-07T18:41:18.909Z  INFO 273 --- [cached.thread-3] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Local MembersView{version=18, members=[MemberInfo{address=[172.18.0.9]:5701, uuid=01fa337e-9acf-464b-a151-31f185c9fe4c, cpMemberUUID=null, liteMember=false, memberListJoinVersion=16}, MemberInfo{address=[172.18.0.6]:5701, uuid=243cf81e-0ce0-4f6b-8253-f21621f67727, cpMemberUUID=null, liteMember=false, memberListJoinVersion=18}]} with suspected members: [Member [172.18.0.9]:5701 - 01fa337e-9acf-464b-a151-31f185c9fe4c] and initial addresses to ask: [] 
2025-12-07T18:41:18.911Z  INFO 273 --- [cached.thread-1] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID 01fa337e-9acf-464b-a151-31f185c9fe4c 
2025-12-07T18:41:18.913Z  INFO 273 --- [cached.thread-1] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:19} [__Member [172.18.0.6]:5701 - 243cf81e-0ce0-4f6b-8253-f21621f67727 this_]_ 
2025-12-07T18:41:18.914Z  INFO 273 --- [cached.thread-1] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Mastership is claimed with: MembersView{version=19, members=[MemberInfo{address=[172.18.0.6]:5701, uuid=243cf81e-0ce0-4f6b-8253-f21621f67727, cpMemberUUID=null, liteMember=false, memberListJoinVersion=18}]} 
2025-12-07T18:41:18.915Z  INFO 273 --- [tform.migration] c.h.i.p.InternalPartitionService         : [172.18.0.6]:5701 [dev] [5.5.0] Fetching partition tables from cluster to determine the most recent one... Local stamp: -887446838781800830 
2025-12-07T18:41:18.915Z  INFO 273 --- [tform.migration] c.h.i.p.InternalPartitionService         : [172.18.0.6]:5701 [dev] [5.5.0] Most recent partition table is determined. 
2025-12-07T18:41:18.915Z  INFO 273 --- [tform.migration] c.h.i.p.InternalPartitionService         : [172.18.0.6]:5701 [dev] [5.5.0] Applying the most recent of partition state... 
2025-12-07T18:41:18.916Z  INFO 273 --- [cached.thread-3] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: 01fa337e-9acf-464b-a151-31f185c9fe4c 
2025-12-07T18:41:18.916Z  WARN 273 --- [tform.migration] c.h.i.p.InternalPartitionService         : [172.18.0.6]:5701 [dev] [5.5.0] Following unknown addresses are found in partition table sent from master[[172.18.0.6]:5701]. (Probably they have recently joined or left the cluster.) {__[172.18.0.9]:5701 - 01fa337e-9acf-464b-a151-31f185c9fe4c_} 
2025-12-07T18:41:18.946Z  INFO 273 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T18:42:21.822Z  INFO 273 --- [.IO.thread-in-2] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:46233 
2025-12-07T18:42:21.912Z  INFO 273 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:20} [__Member [172.18.0.6]:5701 - 243cf81e-0ce0-4f6b-8253-f21621f67727 this__Member [172.18.0.9]:5701 - d15e12c7-1b9f-422a-a9cf-47e2da12e05c_]_ 
2025-12-07T18:42:22.171Z  INFO 273 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T18:42:22.277Z  INFO 273 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 18:42:22 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=549) 
2025-12-07T18:50:07.109Z  INFO 273 --- [.IO.thread-in-2] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Connection[id=2, /172.18.0.6:5701->/172.18.0.9:46233, qualifier=null, endpoint=[172.18.0.9]:5701, remoteUuid=d15e12c7-1b9f-422a-a9cf-47e2da12e05c, alive=false, connectionType=MEMBER, planeIndex=0] closed. Reason: Connection closed by the other side 
2025-12-07T18:50:07.112Z  INFO 273 --- [ached.thread-16] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:50:07.113Z  INFO 273 --- [ached.thread-16] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:50:07.231Z  INFO 273 --- [ached.thread-16] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:50:07.231Z  INFO 273 --- [ached.thread-16] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:50:07.352Z  INFO 273 --- [ached.thread-16] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:50:07.353Z  INFO 273 --- [ached.thread-16] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:50:07.468Z  INFO 273 --- [cached.thread-5] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Connecting to /172.18.0.9:5701, timeout: 10000, bind-any: true 
2025-12-07T18:50:07.469Z  INFO 273 --- [cached.thread-5] c.h.i.server.tcp.TcpServerConnector      : [172.18.0.6]:5701 [dev] [5.5.0] Could not connect to: /172.18.0.9:5701. Reason: IOException[Connection refused to address /172.18.0.9:5701] 
2025-12-07T18:50:07.469Z  WARN 273 --- [cached.thread-5] .h.i.s.t.TcpServerConnectionErrorHandler : [172.18.0.6]:5701 [dev] [5.5.0] Removing connection to endpoint [172.18.0.9]:5701 Cause => java.io.IOException {Connection refused to address /172.18.0.9:5701}, Error-Count: 5 
2025-12-07T18:50:07.469Z  INFO 273 --- [cached.thread-5] c.h.i.cluster.impl.MembershipManager     : [172.18.0.6]:5701 [dev] [5.5.0] Removing Member [172.18.0.9]:5701 - d15e12c7-1b9f-422a-a9cf-47e2da12e05c 
2025-12-07T18:50:07.470Z  INFO 273 --- [cached.thread-5] c.h.i.p.impl.PartitionStateManagerImpl   : [172.18.0.6]:5701 [dev] [5.5.0] Storing snapshot of partition assignments while removing UUID d15e12c7-1b9f-422a-a9cf-47e2da12e05c 
2025-12-07T18:50:07.471Z  INFO 273 --- [ached.thread-12] c.h.t.TransactionManagerService          : [172.18.0.6]:5701 [dev] [5.5.0] Committing/rolling-back live transactions of [172.18.0.9]:5701, UUID: d15e12c7-1b9f-422a-a9cf-47e2da12e05c 
2025-12-07T18:50:07.471Z  INFO 273 --- [cached.thread-5] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:1, ver:21} [__Member [172.18.0.6]:5701 - 243cf81e-0ce0-4f6b-8253-f21621f67727 this_]_ 
2025-12-07T18:50:07.484Z  INFO 273 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Partition balance is ok, no need to repartition. 
2025-12-07T18:50:45.236Z  INFO 273 --- [.IO.thread-in-1] c.h.i.server.tcp.TcpServerConnection     : [172.18.0.6]:5701 [dev] [5.5.0] Initialized new cluster connection between /172.18.0.6:5701 and /172.18.0.9:55153 
2025-12-07T18:50:48.060Z  INFO 273 --- [ration.thread-0] c.h.internal.cluster.ClusterService      : [172.18.0.6]:5701 [dev] [5.5.0] __Members {size:2, ver:22} [__Member [172.18.0.6]:5701 - 243cf81e-0ce0-4f6b-8253-f21621f67727 this__Member [172.18.0.9]:5701 - 0d8e2256-96a4-4b79-8e3a-422a22d57036_]_ 
2025-12-07T18:50:48.312Z  INFO 273 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] Repartitioning cluster data. Migration tasks count: 271 
2025-12-07T18:50:48.406Z  INFO 273 --- [tform.migration] c.h.i.p.impl.MigrationManagerImpl        : [172.18.0.6]:5701 [dev] [5.5.0] All migration tasks have been completed. (repartitionTime=Sun Dec 07 18:50:48 UTC 2025, plannedMigrations=271, completedMigrations=271, remainingMigrations=0, totalCompletedMigrations=820) 
2025-12-07T19:03:16.803Z DEBUG 273 --- [rm-scheduling-1] org.hibernate.SQL                        : select rs1_0.id,rs1_0.chunk_count,rs1_0.columns,rs1_0.created_by,rs1_0.created_date,rs1_0.expires_at,rs1_0.last_modified_by,rs1_0.last_modified_date,rs1_0.name,rs1_0.preview_columns,rs1_0.row_count,rs1_0.storage_format,rs1_0.storage_uri,rs1_0.ttl_days from result_set rs1_0 where rs1_0.expires_at<? 
2025-12-07T20:04:26.515Z DEBUG 273 --- [rm-scheduling-1] org.hibernate.SQL                        : select rs1_0.id,rs1_0.chunk_count,rs1_0.columns,rs1_0.created_by,rs1_0.created_date,rs1_0.expires_at,rs1_0.last_modified_by,rs1_0.last_modified_date,rs1_0.name,rs1_0.preview_columns,rs1_0.row_count,rs1_0.storage_format,rs1_0.storage_uri,rs1_0.ttl_days from result_set rs1_0 where rs1_0.expires_at<? 
2025-12-07T21:04:32.166Z DEBUG 273 --- [rm-scheduling-1] org.hibernate.SQL                        : select rs1_0.id,rs1_0.chunk_count,rs1_0.columns,rs1_0.created_by,rs1_0.created_date,rs1_0.expires_at,rs1_0.last_modified_by,rs1_0.last_modified_date,rs1_0.name,rs1_0.preview_columns,rs1_0.row_count,rs1_0.storage_format,rs1_0.storage_uri,rs1_0.ttl_days from result_set rs1_0 where rs1_0.expires_at<? 
2025-12-07T22:04:24.971Z DEBUG 273 --- [rm-scheduling-1] org.hibernate.SQL                        : select rs1_0.id,rs1_0.chunk_count,rs1_0.columns,rs1_0.created_by,rs1_0.created_date,rs1_0.expires_at,rs1_0.last_modified_by,rs1_0.last_modified_date,rs1_0.name,rs1_0.preview_columns,rs1_0.row_count,rs1_0.storage_format,rs1_0.storage_uri,rs1_0.ttl_days from result_set rs1_0 where rs1_0.expires_at<? 
2025-12-07T23:05:14.056Z DEBUG 273 --- [rm-scheduling-1] org.hibernate.SQL                        : select rs1_0.id,rs1_0.chunk_count,rs1_0.columns,rs1_0.created_by,rs1_0.created_date,rs1_0.expires_at,rs1_0.last_modified_by,rs1_0.last_modified_date,rs1_0.name,rs1_0.preview_columns,rs1_0.row_count,rs1_0.storage_format,rs1_0.storage_uri,rs1_0.ttl_days from result_set rs1_0 where rs1_0.expires_at<? 
